%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}



\cite{Mogensen.2017}
Compiler: Translate (high-level) programming language into machine language

Different phases for writing a compiler, phases are processed sequently
\section{TPTP Language}\label{sec:BackgroundTPTP}
The \acf{TPTP} is a library of problems for automated \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its grammar is specified in an \ac{EBNF}. \cite{Sut17}\\
The \ac{EBNF} \cite{EBNF} is a todo and is used to describe \acp{CFG}.

%\section{\acf{BNF}}\label{sec:BackgroundBNF}

\section{Compiler}\label{sec:BackgroundCompiler}

\cite{Mogensen.2017}
Compiler: Translate (high-level) programming language into machine language

Different phases for writing a compiler, phases are processed sequently

\section{Lexer}\label{sec:BackgroundLexer}


Lexing or a so called lexical analysis is the division of input into units so called tokens \cite{LexYacc.1992}. Tokens are for example variable names or keywords.
The input is a string containing a sequence of characters, the ouput is a sequence of tokens. 
Afterwards, the ouput can be used for further processing e.g. \ref{sec:BackgroundYacc}.

%A lexer takes a set of characters and tries to match them with a token. 

%This can lead to reading the same sequence of characters multiple times until the matching token has been identified. 

A lexer represents an automaton recognizing the specified language TODO \ref{BackgroundRegExp}.

A lexer seperates the given input in order to divide it into tokens. 

Therefore, the lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input. \cite{Mogensen.2017}

A simple approach to build a lexer to to build an automaton for each token definition and then test to which automata the input correspondends.
However, this would be slow as all automatas need to be passed through in the worst case.
Therefore, it is convenient to build a single automaton that tests each token simultaneously.
This automata can be build by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.

It is possible that final states overlap as a consequence of one token being a subset of another token.
For solving such conflicts a precendence of tokens can be declared. Usually the token that is being definded the earliest has a higher precende and thus will be chosen if multiple tokens fit the input. \cite{Mogensen.2017}

The input can also be seperated using the longest input that matches any token is chosen. \cite{Mogensen.2017}
This convention has higher precedence than any previously defined precedences.

\subsubsection{Lexer Generator}

Due to the given complexity, a lexer is often generated by a lexer generator and not written manually.

A lexer generator takes a specification of tokens as input and generates the lexer automatically. 

\subsubsection{Regular expression and formal language}\label{BackgroundRegExp}

The specification of tokens is usually written using regular expressions. 
A regular expression represents a set of strings and describes a formal language. 
A formal language defines a set of words belongig to the language.
These words consist of a finite sequence of characters that on the other hand describe the alphabet of the language.

\subsubsection{Shorthands}
Shorthands are common to simplify a regular expression.
For example all alphabetic letters in lower and upper case are combined and represented by [a-zA-Z].
The same principle can be also be applied to represent a set of numbers.
However, using not clearly defined intervals e.g. [0-b] is not common as it has different interpretations by different lexer generators and thus can lead to mistakes. \cite{Mogensen.2017}


%Input: description of tokens - lex specification, regular expressions]\cite{LexYacc.1992}
%Output: routine that identifies those tokens \cite{LexYacc.1992}


%Each token corresponds to a symbol in the programming language

%A lexer takes a string containing a sequence of chararacters as input and divides this input into units so called tokens. 

\section{Parser}\label{sec:BackgroundParser}
\subsection{Yacc}\label{sec:BackgroundYacc}

Building a syntax tree out of the generated tokens \cite{Mogensen.2017}

-syntax errors
-parser generation

Parsing: establish relationship among tokens \cite{LexYacc.1992}
Grammar: list of rules that defines the relationships \cite{LexYacc.1992}

Input: description of grammar \cite{LexYacc.1992}
Output: parser \cite{LexYacc.1992}


\subsection{PLY}\label{sec:BackgroundPLY}

\acf{PLY} \cite{PLY} is an implementation of lex and yacc in python.
[LALR-parsing]
consists of lex.py and yacc.py

lex.py tokenizes an input string


\section{Python?}\label{sec:BackgroundPython}
\subsection{Packages}\label{sec:BackgroundPackages}
\subsubsection{PyQT}\label{sec:BackgroundPyQT}