%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}
This chapter introduces the technologies and background that will be utilised in the following chapters. First, an introduction into the \ac{TPTP} language is given. Then, formal grammars and the \ac{BNF} are described. Following that, the foundations of lexing and parsing are outlined. Finally, Python and relevant Python modules that are used in the implementation are presented.

\section{TPTP language}\label{sec:BackgroundTPTP}
The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its syntax is specified in an \ac{EBNF}. \cite{Sut17}\\

TODO more detailed

\section{Formal languages}\label{sec:BackgroundFormalLanguage}

An alphabet is a finite, nonempty set of symbols usually represented by $\sum$. An example is the binary alphabet $\sum = \{0,1\}$.
A string is a finite sequence of symbols from some alphabet. For example the string $101$ is a string from the binary alphabet $\sum = \{0,1\}$.
A language is set of strings. If $\sum$ is an alphabet, then    $L(\sum)$ is a language over $\sum$. \cite{AutomataTheory.2007}
- Vocabulary

In summary a formal language describes a set of words belonging to a language. These words are built over the alphabet of the language.

\subsection{Finite automata}\label{sec:BackgroundAutomata}

\subsection{Regular expression}\label{sec:BackgroundRegEx}
%A regular expression describes a formal language and can be described by a finite automata.
%Shorthands are common to simplify a regular expression.
%For example all alphabetic letters in lower and upper case are combined and represented by [a-zA-Z].
%The same principle can be also be applied to represent a set of numbers.
%However, using not clearly defined intervals e.g. [0-b] is not common as it has different interpretations by different lexer generators and thus can lead to mistakes. \cite{Mogensen.2017}

A regular expression is an algebraic description of a regular/formal language. While finite automata also describe languages, automata cannot denote languages. Regular expressions declare strings that are part of the language. \cite{AutomataTheory.2007} 
For example the regular expression \textbf{10+1*} denotes the language consisting of a single 1 followed by a single 0 or any number of 1's. 


\subsection{Formal grammars}\label{sec:BackgroundGrammar}
Unlike regular expressions, grammars not only describe a language but also define a structure among the words of a language.\\
A grammar is a list of rules that defines the relationships among tokens \cite{LexYacc.1992}.
These rules are also referred to as production rules.
Given a start symbol, this symbol can be replaced by other symbols using the production rules.
Using a recursive notation, production rules define derivations for symbols. The derived symbols can then once again be replaced until the derivation is a terminal symbol.  
Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language is build by the set of terminal symbols.
Nonterminal symbols however can be further derived and build  merged with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 
\\
- Beispiel

\subsubsection{Context-free grammar}

\subsubsection{Reduced grammars}
Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}. \\
Given the set of terminal symbols $\sum$, a nonterminal symbol $\xi$ is called terminating if there are productions $\xi \underrightarrow{*} z$ so that $\xi$ can be derivated to $z$ and $z \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is terminating if there exist production rules so that  $\xi$ can be replaced by terminal symbols. \cite{Cremers75}\\
Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $\xi$ is called reachable if there are production rules $S \underrightarrow{*} u\xi v$ so that $S$ can be derivated to $u\xi v$ and $u,v \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is reachable if there exist production rules so that the start symbol can be replaced by a symbol containing $\xi$. \cite{Cremers75}




\section{\acf{BNF}}\label{sec:BackgroundBNF}
The  \acf{BNF} is a language to describe context-free grammars.
In the \acf{BNF} nonterminal symbols are distinguished from terminal symbols by being enclosed by  angle brackets, e. g. <$TPTP\_File$> denotes the nonterminal symbol $TPTP\_File$.
Productions are described using the $"::="$ symbol and alternatives are specified using the $"|"$ symbol. \cite{BNF.1964} 
An example for a \ac{BNF} production would be $TPTP\_File$ $::=$ <$TPTP\_Input$> $|$ <$comment$>.
Using this pattern of notation whole grammars can be specified.\\
The \ac{EBNF} extends the \ac{BNF} by with following rules:

\begin{itemize}%[noitemsep]
	\item optional expressions are surrounded by square brackets.
	\item repetition is denoted by curly brackets.
	\item parentheses are used for grouping.
	\item terminals are enclosed in quotation marks.
\end{itemize}
\label{itemize:BackgroundBNF}
\cite{EBNF.1977}

\section{Lexing}\label{sec:BackgroundLexer}
Lexing or a so called lexical analysis is the division of input into units so called tokens \cite{LexYacc.1992}. Tokens are for example variable names or keywords.
The input is a string containing a sequence of characters, the ouput is a sequence of tokens. 
Afterwards, the ouput can be used for further processing like parsing.
A lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input. \cite{Mogensen.2017}\\
A simple approach to build a lexer would be building an automaton for each token definition and then test to which automata the input corresponds.\\
However, this would be inefficient because in the worst case the input needs to pass all automata before the belonging automata is identified.
More suitable is building a single automata that tests each token simultaneously.
This automata can be build by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.\\
Potentially final states overlap as a consequence of one token being a subset of another token.
For solving such conflicts a precedence of tokens can be declared. Usually the token that is being defined first has a higher precedence and thus will be chosen if multiple tokens fit the input. \cite{Mogensen.2017}\\
Furthermore, a lexer is separating the input in order to divide it into tokens.
Per convention the longest input that matches any token is chosen. \cite{Mogensen.2017}

Instead of writing a lexer manually it is often generated by a lexer generator. A lexer generator takes a specification of tokens as input and generates the lexer automatically. 
The specification is usually written using regular expressions. 

%Input: description of tokens - lex specification, regular expressions]\cite{LexYacc.1992}
%Output: routine that identifies those tokens \cite{LexYacc.1992}

%Tokens are for example variable names or keywords. 

%Each token corresponds to a symbol in the programming language

%A lexer takes a string containing a sequence of chararacters as input and divides this input into units so called tokens. 

\section{Parsing}\label{sec:BackgroundParser}
The aim of parsing is to establish a relationship among tokens generated by a lexer \cite{LexYacc.1992}. For doing so, a parser builds a syntax tree out of the generated tokens \cite{Mogensen.2017}.\\
Similar to lexers, parsers can be generated automatically.
Therefore a parser generator takes as input a description of the relationship among tokens in form of a formal grammar (see ). The output is the generated parser. \cite{LexYacc.1992}

During the syntax analysis a parser takes a string of tokens and forms a syntax tree with this construct by finding the matching derivations. The matching derivation can be found by using different approaches for examble random guessing (predictive parsing) or LR parsing.
Input: description of grammar \cite{LexYacc.1992}
Output: parser \cite{LexYacc.1992}

-bottom up (LR parsing):
parser takes inputs and searches for production where input is on the right side of a production rule and then replaces it by the left side
-top down (predictive parsing):
parser takes input and searches for production where input is on the left side of a production rule

\section{Python}\label{sec:BackgroundPython}
\subsection{PLY}\label{sec:BackgroundPythonPLY}

\acf{PLY} \cite{PLY} is an implementation of lex and yacc in python.
[LALR-parsing]
consists of lex.py and yacc.py

lex.py tokenizes an input string\
\subsection{PyQt}\label{sec:BackgroundPytonPyQt}
PyQt is a Python binding for the cross-platform GUI framework Qt \cite{PyQt}.
It is licensed under the GNU GPL version 3.


tkinter
\subsection{argparse}\label{sec:BackgroundArgparse}
The python module argparse is a module for creating command line interfaces.
It provides the means to specify input arguments and \cite{argparse} automatically creates help and usage messages.
It also checks if the given arguments are valid.
After specifying input arguments, the module will automatically create a parser for the specified arguments.