%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}

\cite{Mogensen.2017}
Compiler: Translate (high-level) programming language into machine language

Different phases for writing a compiler, phases are processed sequently
\section{TPTP Language}\label{sec:BackgroundTPTP}
The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its grammar is specified in an \ac{EBNF}. \cite{Sut17}\\

\section{\acf{BNF}}\label{sec:BackgroundBNF}
The  \acf{BNF} is a language to describe context-free grammars.
In the \acf{BNF} nonterminal symbols are distinguished from terminal symbols by being enclosed by  angle brackets, e. g. <$TPTP\_File$> denotes the nonterminal symbol $TPTP\_File$.
Productions are described using the $"::="$ symbol and alternatives are specified using the $"|"$ symbol. \cite{BNF.1964} 
An example for a \ac{BNF} production would be $TPTP\_File$ $::=$ <$TPTP\_Input$> $|$ <$comment$>.
Using this pattern of notation whole grammars can be specified.\\
The \ac{EBNF} extends the \ac{BNF} by with following rules:

\begin{itemize}%[noitemsep]
	\item optional expressions are sorrounded by square brackets.
	\item repetition is denoted by curly brackets.
	\item parentheses are used for grouping.
	\item terminals are enclosed in quotation marks.
\end{itemize}
\label{itemize:BackgroundBNF}
\cite{EBNF.1977}


Different phases for writing a compiler, phases are processed sequently

\section{Grammar}\label{sec:BackgroundGrammar}
Unlike regular expressions, grammars not only describe a language but also define a structure among the words of a language(?)(->additionally defines structure on the strings in the language it defines).
Lexing or a so called lexical analysis is the division of input into units so called tokens \cite{LexYacc.1992}. 
The input is a string containing a sequence of characters. 
Often, the lexer is generated by a lexer generator and not written manually.
When writing the lexer manually TODO
A lexer generator takes a specification of tokens as input and generates the lexer automatically. 
The specification is usually written using regular expressions. 
A regular expression describes a formal language and can be described by a finite automata.
A formal language describes a set of words belongig to the language.
These words are built over the alphabet of the language.
Shorthands are common to simplify a regular expression.
For example all alphabetic letters in lower and upper case are combined and represented by [a-zA-Z].
The same principle can be also be applied to represent a set of numbers.
However, using not clearly defined intervals e.g. [0-b] is not common as it has different interpretations by different lexer generators and thus can lead to mistakes. \cite{Mogensen.2017}

A grammar is a list of rules that defines the relationships among tokens \cite{LexYacc.1992}.
These rules are also referred to as production rules.
Given a start symbol, this symbol can be replaced by other symbols using the production rules.
Using a recursive notation, production rules define derivations for symbols. The derived symbols can then once again be replaced until the derivation is a terminal symbol.  
Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language build the set of terminal symbols.
Nonterminal symbols however can be further derived and build  merged with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 

\subsubsection{Reduced Grammar}
Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}.
 
Given the set of terminal symbols $\sum$, a nonterminal symbol $\xi$ is called terminating if there are productions $\xi \underrightarrow{*} z$ so that $\xi$ can be derivated to $z$ and $z \epsilon \sum$*. 
In other words, a nonterminal symbol $\xi$ is terminating if there exist production rules so that  $\xi$ can be replaced by terminal symbols. \cite{Cremers75}

Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $\xi$ is called reachable if there are production rules $S \underrightarrow{*} u\xi v$ so that $S$ can be derivated to $u\xi v$ and $u,v \epsilon \sum$*. 
In other words, a nonterminal symbol $\xi$ is reachable if there exist production rules so that the start symbol can be replaced by a symbol containing $\xi$. \cite{Cremers75}

\subsubsection{Context-free grammar}


\section{Lexer}\label{sec:BackgroundLexer}
Lexing or a so called lexical analysis is the division of input into units so called tokens \cite{LexYacc.1992}. Tokens are for example variable names or keywords.
The input is a string containing a sequence of characters, the ouput is a sequence of tokens. 
Afterwards, the ouput can be used for further processing e.g. \ref{sec:BackgroundYacc}.
A lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input. \cite{Mogensen.2017}

A simple approach to build a lexer to to build an automata for each token definition and then test to which automata the input correspondends.
However, this would be slow as all automatas need to be passed through in the worst case.
Therefore, it is convenient to build a single automata that tests each token simultaneously.
This automata can be build by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.

It is possible, that final states overlap as a consequence of one token being a subset of another token.
For solving such conflicts a precendence of tokens can be declared. Usually the token that is being definded the earliest has a higher precende and thus will be chosen if multiple tokens fit the input. \cite{Mogensen.2017}

Another task of the lexer is seperating the input in order to divide it into tokens.
Per convention the longest input that matches any token is chosen. \cite{Mogensen.2017}


%Input: description of tokens - lex specification, regular expressions]\cite{LexYacc.1992}
%Output: routine that identifies those tokens \cite{LexYacc.1992}

%Tokens are for example variable names or keywords. 

%Each token corresponds to a symbol in the programming language

%A lexer takes a string containing a sequence of chararacters as input and divides this input into units so called tokens. 

\section{Parser}\label{sec:BackgroundParser}
Building a syntax tree out of the generated tokens \cite{Mogensen.2017}

Similar to lexers, parsers can be generated automatically.
Therefore a parser generator takes as input a description of the relationship among tokens in form of a grammar. The output is the generated parser. \cite{LexYacc.1992}
 
\subsubsection{Syntax analysis}
Parsing: establish relationship among tokens \cite{LexYacc.1992}
Grammar: list of rules that defines the relationships \cite{LexYacc.1992}

In this phase a parser will take a string of tokens and form a syntax tree with this construct by finding the matching derivations. The matching derivation can be found by using different approaches for examble random guessing (predictive parsing) or LR parsing.
Input: description of grammar \cite{LexYacc.1992}
Output: parser \cite{LexYacc.1992}

-bottom up (LR parsing):
parser takes inputs and searches for production where input is on the right side of a production rule and then replaces it by the left side
-top down (predictive parsing):
parser takes input and searches for production where input is on the left side of a production rule


\section{PLY}\label{sec:BackgroundPLY}

\acf{PLY} \cite{PLY} is an implementation of lex and yacc in python.
[LALR-parsing]
consists of lex.py and yacc.py

lex.py tokenizes an input string\label{sec:BackgroundNDAutomata}

\section{Python?}\label{sec:BackgroundPython}