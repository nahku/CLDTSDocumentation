%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}

This chapter introduces the technologies and background that will be utilized in the following chapters. First, an introduction into the \ac{TPTP} language is given. Then, formal grammars and the \ac{BNF} are described. Following that, the foundations of lexing and parsing are outlined. Finally, Python and relevant Python modules that are used in the implementation are presented.

\section{TPTP language}\label{sec:BackgroundTPTP}

The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its syntax is specified in an \ac{EBNF}. \cite{Sut17}\\

TODO more detailed

\section{Formal languages}\label{sec:BackgroundFormalLanguage}

This section introduces terms and concepts in the area of formal languages that are part of the basics of \ac{Synplifier}.

\subsection{Alphabet}
An alphabet is a finite, non-empty set of symbols usually represented by $\sum$. An example is the binary alphabet $\sum = \{0,1\}$. \cite{AutomataTheory.2007}

\subsection{String}
A string also called word is a finite sequence of symbols from some alphabet. For example the string \textit{101} is a string from the binary alphabet $\sum = \{0,1\}$.
The set of all strings over an alphabet $\sum$ is denoted as $\sum ^{*}$. \cite{AutomataTheory.2007}

\subsection{Language}
A formal language is a set of words over an alphabet $\sum ^*$. If $\sum$ is an alphabet and $L$ is a subset of $\sum ^*$, then $L$ is a language over $\sum$. \cite{AutomataTheory.2007}

\subsection{Finite automata}\label{sec:BackgroundAutomata}

A finite automaton recognizes words of a language. 
An automaton is composed of states and transitions between states. 
Transitions are executed in response to the input of an automaton. 
States are usually represented by circles and transitions by labelled arrows. 
Two special states are the start and the final/accepting state. 
The start state is the state in which the automaton is in the beginning. 
If the automaton reaches the final state, the word is part of the language the automaton represents. 
It can be distinguished between deterministic and non-deterministic automata. 
Non-deterministic have multiply transitions for the same input and thus it can not be predicted how the automata will operate.  
The concept of finite automata can for example be applied to a lexical analyzer for recognizing tokens (see section \ref{sec:BackgroundLexer}). \cite{AutomataTheory.2007}\\
Figure \ref{fig:FiniteAutomation} shows an automaton consisting of the three states $z\textsubscript{0}, z\textsubscript{1}$ and $z\textsubscript{2}$ and three transitions. The automaton accepts words consisting of a 1, an arbitrary amount of zeros and a 1. 

\begin{figure}[H]
\centering
\begin{tikzpicture}[->, >=stealth', shorten >= 5pt,auto, node distance = 2.5cm, semithick]

\node[initial, state] (R) {z\textsubscript{0}};
\node[state] (S) [right of=R] {z\textsubscript{1}};
\node[state, accepting] (T) [right of=S] {z\textsubscript{2}};

\path (R) edge [below] node {1}(S)
	  (S) edge [loop, above] node{0} (S)
	  (S) edge [below] node {1} (T)
	  ;
\end{tikzpicture}
\caption{Finite automaton}
\label{fig:FiniteAutomation}
\end{figure}

\subsection{Regular expression}\label{sec:BackgroundRegEx}

A regular expression is an algebraic description of a regular/formal language. Regular expressions declare strings that are part of the language and can describe the same words that can also be represented by a finite automaton. 
In comparison to finite automata, the words can be described in an algebraic way. 
Due to the similarities between regular expressions and finite automata, it is possible to convert regular expressions to finite automata and backwards. 
Regular expressions are often used to describe tokens that should be recognized by a lexer (see section \ref{sec:BackgroundLexer}). \cite{AutomataTheory.2007}\\
For example the regular expression \textbf{10+1*} denotes the language consisting of a single 1 followed by a single 0 or any number of 1's. 

\subsection{Formal grammars}\label{sec:BackgroundGrammar}

Formal grammars can generate words of a language and are thus able to describe larger languages than regular expression can describe. 
Grammars process data using a recursive structure.

%-s Ok, aber for allem können sie viel größer Klassen von Sprachen beschreiben!
%-...und wenn ich schon dabei bin: Machen REs das nicht auch? Nur halt anders.


The description of a grammar consists of four components:

\begin{itemize}

\item A set of symbols that defines words of the language. These symbols are called terminal symbols.

\item A set of variables called nonterminal symbols. Each symbol represents a set of strings. 

\item One nonterminal symbol represents the language that is being defined. This nonterminal symbol is called the start symbol. Other nonterminal symbols help to define the language of the start symbol.

\item A set of productions/rules. They represent the recursive definition of the language. Each production consists of: nonterminal symbol that is being (partially) defined in production, production symbol, and a string of terminal symbols and/or variables.
In production terminals are unchanged and nonterminal symbols are substituted by its production body.
- empty production
\end{itemize}

- ambigious grammars
- precedence rules?

\cite{AutomataTheory.2007}

%A grammar is a list of rules that defines the relationships between tokens \cite{LexYacc.1992}.
%These rules are also referred to as production rules.
%Given a start symbol, this symbol can be replaced by other symbols using the production rules.
%Using a recursive notation, production rules define derivations for words. The derived symbols can then once again be replaced until the derivation
%
%s Hier sauber trennen:
%- Die Ableitung ist eine Folge von Regelanwendungen
%- Das abgeleitete Wort
%- Terminale und nicht-terminale Symbole
%Speziell: Das abgeleitete Wort ist im allgemeinen kein Symbol
%(weder terminal noch nicht-terminal), sondern es besteht aus
%Symbolen (und wenn die Ableitung fertig ist, nur aus Terminalsymbolen
%
%is a terminal symbol.  
%Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language is built by the set of terminal symbols.
%Nonterminal symbols however can be further derived and todo ?build  merged? with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 
%\\
%- Beispiel

\subsubsection{Context-free grammars}

A context-free grammar is a grammar that only allows productions rules that are in the form of $V \rightarrow \beta$ with V being a nonterminal symbol and $\beta$ being a sequence of nonterminal and terminal symbol with an arbitrary length. Context-free grammars are often the basis of a parser (see section \ref{sec:BackgroundParser}). \cite{AutomataTheory.2007}

\subsubsection{Reduced grammars}

Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}. \\
Given the set of terminal symbols $\sum$, a nonterminal symbol $\xi$ is called terminating if there are productions $\xi \underrightarrow{*} z$ so that $z$ can be derived from $\xi$ and $z \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is terminating if there exist production rules so that  $\xi$ can be replaced by a string of terminal symbols. \cite{Cremers75}\\
Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $\xi$ is called reachable if there are production rules $S \underrightarrow{*} u\xi v$ so that $S$ can be derived to $u\xi v$ and $u,v \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is reachable if there exist production rules so that the start symbol can be replaced by a word containing $\xi$. \cite{Cremers75}

todo beispiel

\section{\acf{BNF}}\label{sec:BackgroundBNF}

The \acf{BNF} is a language to describe context-free grammars.
In the \acf{BNF} nonterminal symbols are distinguished from terminal symbols by being enclosed by  angle brackets, e. g. <$TPTP\_File$> denotes the nonterminal symbol $TPTP\_File$.
Productions are described using the $"::="$ symbol and alternatives are specified using the $"|"$ symbol. \cite{BNF.1964}
An example for a \ac{BNF} production would be $TPTP\_File$ $::=$ <$TPTP\_Input$> $|$ <$comment$>.
Using this pattern of notation whole grammars can be specified.\\
The \ac{EBNF} extends the \ac{BNF} by with following rules:

\begin{itemize}%[noitemsep]
	\item optional expressions are surrounded by square brackets.
	\item repetition is denoted by curly brackets.
	\item parentheses are used for grouping.
	\item terminals are enclosed in quotation marks.
\end{itemize}
\label{itemize:BackgroundBNF}
\cite{EBNF.1977}

\section{Lexing}\label{sec:BackgroundLexer}

Lexing or a so-called lexical analysis is the division of input into units called tokens \cite{LexYacc.1992}.\\
The input is a string containing a sequence of characters.
The lexer groups characters of the input string into sequences that are meaningful. These sequences are called lexemes. From each lexeme, the lexer produces a token, which consists of the token name and the lexeme string. The token name is an abstract symbol which is used during parsing. \cite{Aho.2007} \\
The tokens are then passed to the parser for syntax analysis.\\
A lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input \cite{Mogensen.2017}.\\
A simple approach to build a lexer would be building an automaton for each token definition and then test to which automata the input corresponds.\\
However, this would be inefficient because in the worst case the input needs to pass all automata before the belonging automata is identified.
More suitable is building a single automaton that tests each token simultaneously.
This automaton can be built by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.\\
Potentially final states overlap as a consequence of one token being a substring of another token. \\
For solving such conflicts, a lexer is separating the input in order to divide it into tokens.
Per convention the lexer chooses the longest input that matches any token. \cite{Mogensen.2017} \\
Furthermore, a precedence of tokens can be declared. Usually the token that is being defined first has a higher precedence and thus will be chosen if possible token matches have the same length. \cite{Mogensen.2017}

Besides of writing a lexer manually it can also be generated by a lexer generator. A lexer generator takes a specification of tokens as input and generates the lexer automatically. 
The specification is usually written using regular expressions. 

\section{Parsing}\label{sec:BackgroundParser}

The aim of parsing is to establish a relationship among tokens generated by a lexer \cite{LexYacc.1992}. For doing so, a parser builds a parse tree out of the generated tokens \cite{Mogensen.2017}.\\
Similar to lexers, parsers can be generated automatically.
A parser generator takes as input a description of the relationship among tokens in form of a formal grammar (see ). The output is the generated parser. \cite{LexYacc.1992}\\
During the syntax analysis a parser takes a string of tokens and forms a syntax tree with this construct by finding the matching derivations. The matching derivation can be found by using different approaches. The approaches that are used in this report will be introduced in the following.

\subsection{Bottom-up parsing}\label{sec:BackgroundParserBottomUp}

In bottom-up parsing a parse tree for an input string is constructed beginning at the leaves working up to the root. The idea is to reduce a string to the start symbol of a grammar, constructing a deviation in reverse.
\cite{Aho.2007}

-at each reduction step specific substring matching body of a production is replaced by the nonterminal at the head of that productions (reduction is reverse step of deviation)
-key decisions are when to reduce and what production to apply

\subsubsection{Shift-reduce parsing}\label{sec:BackgroundParserShiftReduce}

Shift-reduce parsers are one class of bottom-up parsers. In shift-reduce parsing a stack is used to hold grammar symbols and an input buffer holds the rest of the input string.
During parsing the input is scanned from left to right. The parser shifts zero or more input symbols on the stack until it can reduce the elements at the top of the stack. The elements at the top of the stack are reduced to the head of the corresponding production. This procedure is repeated until the stack contains the start symbol and the input buffer is empty or an error is detected. \cite{Aho.2007}\\
There are four actions a shift-reduce parser can perform:
\begin{itemize}%[noitemsep]
	\item \textbf{Shift}: Shift next input symbol on top of stack.
	\item \textbf{Reduce}: todo right end of sting to be reduced must be at top of stack, locate left end of string within the stack and decide with what nonterminal to replace the string.
	\item \textbf{Accept}: Declare that Input was successfully parsed.
	\item \textbf{Error}: Detect a syntax error.
\end{itemize}
\cite{Aho.2007}

-use justified by, handle will appear on top of stack not inside, proof ...

There are two kinds of conflicts that can occur during shift-reduce parsing. A shift-reduce conflict occurs if the parser cannot decide whether to shift or reduce.
A reduce-reduce conflict occurs if the parser cannot decide which of multiple reductions to make.

-these grammars are not in LR(k) class of grammars (non-LR grammars)
for example: ambiguous grammar can never be LR (if else)
\cite{Aho.2007}

\subsubsection{LR($k$) parsing}\label{sec:BackgroundParserLR}

LR($k$) parsing is the most common type of bottom-up parsing. The L stands for left-to-right and R stands for constructing a rightmost derivation in reverse. The parameter $k$ refers to the number of inputs symbols that are used as a lookahead. When $k$ is omitted, $k = 1$ is assumed.
LR-Parsers are complex to implement, but LR parser generators can be used, which significantly reduces the effort for creating an LR-Parser.

-what is rightmost derivation
continue at s 241


\cite{Aho.2007}
-lr p. 241

\subsubsection{LALR parsing}\label{sec:BackgroundParserLALR}

- lookahead lr technique
\cite{Aho.2007}
-p.259

\section{Lex and Yacc}\label{sec:BackgroundLexYacc}

Lex and Yacc are tools for writing lexers and parsers. They often work together, meaning the parser uses the generated tokens from the lexer as input.

\subsection{Lex}\label{sec:BackgroundLex}

Lex is a tool written in C that generates a lexer by specifying regular expressions and belonging code fragments. The specified regular expressions are translated into a program that reads an input stream. The program partitions the input stream into strings matching the regular expressions. Once an expression is recognized the corresponding code is executed. For recognizing expressions, Lex builds a deterministic finite automaton. The automaton chooses the longest possible match if several expressions fit the input. \cite{Lex}

\subsection{Yacc}\label{sec:BackgroundYacc}
Yacc is a tool written in C that can be used for parsing the input of a computer program. 
For generating a parser using Yacc, a specification specified the structure of the input has to be given. Besides the specification, the input also consists of code that is invoked once a structure has been recognized. 
Yacc turns the input specification into a routine that handles the input. This routine calls Lex to get tokens from the input stream and arrange them based on the specification. \cite{Yacc}


\section{Python}\label{sec:BackgroundPython}

Since the use cases are all hand-created grammars of relatively small
size (dozens to thousands of rules), and the algorithms do not have
high complexity, we decided to implement the tool in Python 3. 
Python
is a simple but powerful modern multi-paradigm language with good support and excellent
libraries. Python is easy to learn and its power allows it to create complex applications using Python. Libraries that are used to implement \ac{Synplifier} are introduced in the following sections.

\subsection{PLY}\label{sec:BackgroundPythonPLY}

\acf{PLY} is an implementation of lex and yacc in Python. It is compatible with Python 2 and 3 and is fully implemented in Python. The goal of \ac{PLY} is to offer a pure-Python implementation of lex and yacc. The implementation aims to rebuild the functionalities of lex and yacc and thus includes:
\begin{itemize}
\item Support for LALR parsing (see chapter \ref{sec:BackgroundParserLALR})
\item Support for empty productions
\item Support for ambiguous grammars 
\item Precedence rules
\item Error checking and recovery
\end{itemize}

\ac{PLY} consists of the two modules $lex.py$ and $yacc.py$ that are meant to execute together. For example $yacc.py$ retrieves tokens from the input stream provided by $lex.py$. $Yacc.py$ returns by default an Abstract Syntax Tree. However, it is possible for the user to change the output of $yacc.py$ according to his demands.

\cite{PLY}


\subsection{PyQt}\label{sec:BackgroundPytonPyQt}

PyQt is a Python binding for the cross-platform GUI framework Qt \cite{PyQt}.
It is licensed under the GNU GPL version 3.
- qt is set of c++ libraries and development tools for various things: graphical user interfaces, regular expressions, sql databases, nfc and bluetooth,...
- pyqt implements over 1000 of these classes in python
- QMainWindow \\
- QTreeWidget \\
-pyqt 5

tkinter

\subsection{argparse}\label{sec:BackgroundArgparse}

The Python module argparse is a module for creating command-line argument parsers.
It provides the means to specify input arguments and automatically creates help and usage messages.
It also checks if the given arguments are valid.
From the specified input arguments, the module will automatically create a parser for the specified arguments. \cite{argparse} 

\subsection{Beautiful Soup}\label{sec:BackgroundBeautifulSoup}

Beautiful Soup is a Python library for extracting data out of HTML and XML files. That makes it especially useful for getting information from web pages. Beautiful Soup is based on Python parses like $lxml$ and $html5lib$ and parses the files in order to build a parse tree and retrieve desired informations. \cite{BeautifulSoup,BeautifulSoupDoku}