%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}



\cite{Mogensen.2017}
Compiler: Translate (high-level) programming language into machine language

Different phases for writing a compiler, phases are processed sequently
\section{TPTP Language}\label{sec:BackgroundTPTP}
The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its grammar is specified in an \ac{EBNF}. \cite{Sut17}\\
The \ac{EBNF} \cite{EBNF} is a todo and is used to describe \acp{CFG}.

%\section{\acf{BNF}}\label{sec:BackgroundBNF}

\section{Compiler}\label{sec:BackgroundCompiler}

\cite{Mogensen.2017}
Compiler: Translate (high-level) programming language into machine language

Different phases for writing a compiler, phases are processed sequently

\section{Grammar}\label{sec:BackgroundGrammar}

Unlike regular expressions, grammars not only describe a language but also define a structure among the words of a language(?)(->additionally defines structure on the strings in the language it defines).

A grammar is a list of rules that defines the relationships among tokens \cite{LexYacc.1992}.
These rules are also referred to as production rules.
Given a start symbol, this symbol can be replaced by other symbols using the production rules.
Using a recursive notation, production rules define derivations for symbols. The derived symbols can then once again be replaced until the derivation is a terminal symbol.  
Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language build the set of terminal symbols.
Nonterminal symbols however can be further derived and build  merged with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 

\subsubsection{Reduced Grammar}
Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}.
 
Given the set of terminal symbols $\sum$, a nonterminal symbol $\xi$ is called terminating if there are productions $\xi \underrightarrow{*} z$ so that $\xi$ can be derivated to $z$ and $z \epsilon \sum$*. 
In other words, a nonterminal symbol $\xi$ is terminating if there exist production rules so that  $\xi$ can be replaced by terminal symbols. \cite{Cremers75}

Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $\xi$ is called reachable if there are production rules $S \underrightarrow{*} u\xi v$ so that $S$ can be derivated to $u\xi v$ and $u,v \epsilon \sum$*. 
In other words, a nonterminal symbol $\xi$ is reachable if there exist production rules so that the start symbol can be replaced by a symbol containing $\xi$. \cite{Cremers75}

\subsubsection{Context-free grammar}


\section{Lexer}\label{sec:BackgroundLexer}
Lexing or a so called lexical analysis is the division of input into units so called tokens \cite{LexYacc.1992}. Tokens are for example variable names or keywords.
The input is a string containing a sequence of characters, the ouput is a sequence of tokens. 
Afterwards, the ouput can be used for further processing e.g. \ref{sec:BackgroundYacc}.

%A lexer takes a set of characters and tries to match them with a token. 

%This can lead to reading the same sequence of characters multiple times until the matching token has been identified. 

A lexer represents an automaton recognizing the specified language TODO \ref{BackgroundRegExp}.

A lexer seperates the given input in order to divide it into tokens. 

Therefore, the lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input. \cite{Mogensen.2017}

A simple approach to build a lexer to to build an automaton for each token definition and then test to which automata the input correspondends.
However, this would be slow as all automatas need to be passed through in the worst case.
Therefore, it is convenient to build a single automaton that tests each token simultaneously.
This automata can be build by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.

It is possible that final states overlap as a consequence of one token being a subset of another token.
For solving such conflicts a precendence of tokens can be declared. Usually the token that is being definded the earliest has a higher precende and thus will be chosen if multiple tokens fit the input. \cite{Mogensen.2017}

The input can also be seperated using the longest input that matches any token is chosen. \cite{Mogensen.2017}
This convention has higher precedence than any previously defined precedences.

\subsubsection{Lexer Generator}

Due to the given complexity, a lexer is often generated by a lexer generator and not written manually.

A lexer generator takes a specification of tokens as input and generates the lexer automatically. 

\subsubsection{Regular expression and formal language}\label{BackgroundRegExp}

The specification of tokens is usually written using regular expressions. 
A regular expression represents a set of strings and describes a formal language. 
A formal language defines a set of words belongig to the language.
These words consist of a finite sequence of characters that on the other hand describe the alphabet of the language.

%\subsubsection{Shorthands}
%Shorthands are common to simplify a regular expression.
%For example all alphabetic letters in lower and upper case are combined and represented by [a-zA-Z].
%The same principle can be also be applied to represent a set of numbers.
%However, using not clearly defined intervals e.g. [0-b] is not common as it has different interpretations by different lexer generators and thus can lead to mistakes. \cite{Mogensen.2017}


%Input: description of tokens - lex specification, regular expressions]\cite{LexYacc.1992}
%Output: routine that identifies those tokens \cite{LexYacc.1992}


%Each token corresponds to a symbol in the programming language

%A lexer takes a string containing a sequence of chararacters as input and divides this input into units so called tokens. 

\section{Parser}\label{sec:BackgroundParser}
%\subsection{Yacc}\label{sec:BackgroundYacc}

Parsing describes the process of establishing a relationship among previously generated tokens \cite{LexYacc.1992}.
The established relationship is described in a syntax tree \cite{Mogensen.2017}. The leaves of the syntax tree are the tokens \cite{Mogensen.2017}.

Similar to lexers, parsers can be generated automatically.
Therefore a parser generator takes as input a description of the relationship among tokens in form of a grammar. The output is the generated parser. \cite{LexYacc.1992}
 
\subsubsection{Syntax analysis}

In this phase a parser will take a string of tokens and form a syntax tree with this construct by finding the matching derivations. The matching derivation can be found by using different approaches for examble random guessing (predictive parsing) or LR parsing.

-bottom up (LR parsing):
parser takes inputs and searches for production where input is on the right side of a production rule and then replaces it by the left side
-top down (predictive parsing):
parser takes input and searches for production where input is on the left side of a production rule


\section{PLY}\label{sec:BackgroundPLY}

\acf{PLY} \cite{PLY} is an implementation of lex and yacc in python.
[LALR-parsing]
consists of lex.py and yacc.py

lex.py tokenizes an input string


\section{Python?}\label{sec:BackgroundPython}
\subsection{Packages}\label{sec:BackgroundPackages}
\subsubsection{PyQT}\label{sec:BackgroundPyQT}