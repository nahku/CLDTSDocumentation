%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}

This chapter introduces the technologies and background that will be utilized in the following chapters. First, an introduction into formal grammars is given and the \acf{BNF} is described. Following that, the \ac{TPTP} language is introduced. Then the foundations of lexing and parsing are outlined. After that, Python and relevant Python packages that are used in the implementation of \ac{Synplifier} are presented.

\section{Formal languages}\label{sec:BackgroundFormalLanguage}

This section introduces terms and concepts in the area of formal languages that are necessary to understand \ac{Synplifier}.

\subsection{Alphabet}
An alphabet is a finite, non-empty set of symbols usually represented by the uppercase sigma $\sum$.
An example is the binary alphabet $\sum = \{0,1\}$. \cite{AutomataTheory.2007}

\subsection{String}
A string, also called word, is a finite sequence of symbols from some alphabet. For example the string \textit{101} is a string from the binary alphabet $\sum = \{0,1\}$.
The set of all strings over an alphabet $\sum$ is denoted as $\sum ^{*}$. \cite{AutomataTheory.2007}

\subsection{Language}
If $\sum$ is an alphabet and $L$ is a subset of $\sum ^*$, then $L$ is a language over $\sum$ \cite{AutomataTheory.2007}.
If sigma is the \ac{ASCII} alphabet $\sum = \{a-zA-Z,0-9,\enspace,!,=,\#,\%,\$,\&,',(,),*,+,,,-,.,/,:,;,<,>,=,?,@,[,],\backslash ,\textasciicircum ,\textunderscore ,`,\{,\},\mid,\sim\}$, then for example the language where words can consist of $a-zA-Z$ is one language over the \ac{ASCII} alphabet.\\
In the example the abbreviations $a-zA-Z$ and $0-9$ are used. The first abbreviation represents all alphabetic letters in lower and upper case. The second abbreviation represents all one-digit numbers.

\subsection{Finite automata}\label{sec:BackgroundAutomata}
A finite automaton can recognize words of a language.
The language represented by an automaton is the set of all accepted words.
An automaton consists of a set of states, a set of input symbols and a transition function.
The transition function takes a state and an input symbol as input and returns a state based on these input arguments.
States are usually represented graphically by circles and transitions by labelled arrows.
There are two special states: The start state and final state(s).
The start state is the state in which the automaton starts processing the input symbols.
It is represented graphically by an arrow pointing to the start state.
Final states are represented graphically by a second circle embedded in the circle of a state.
Starting from the start state the automaton processes the first input symbol by evaluating the belonging transition function that takes the start state and the first input symbol as arguments. The automaton continues to process the input data by evaluating transition functions until the automaton has read every input symbol. The input word is part of the language the automaton represents if the automaton has reached a final state. If the automaton has not reached a final state, the input words is not part of the language the automaton represents.\\
Deterministic and non-deterministic automata can be distinguished.
Non-deterministic automata can have multiple transitions for the same input.
Deterministic automata have unambiguous transitions for a given input.
The concept of finite automata be applied to a lexical analysers for recognizing tokens (see section \ref{sec:BackgroundLexer}). \cite{AutomataTheory.2007}\\
Figure \ref{fig:FiniteAutomaton} shows an automaton consisting of the set of states $\{z\textsubscript{0}, z\textsubscript{1}, z\textsubscript{2}\}$ and several transitions. For convenience not all transitions are printed.
If the first input symbol would be a zero the automaton would transit in a so called error state which can not be left regardless the input symbols.\\
The automaton accepts words consisting of a one, an arbitrary amount of zeros and a one after that.

\begin{figure}[H]
\centering
\begin{tikzpicture}[->, >=stealth', shorten >= 5pt,auto, node distance = 2.5cm, semithick]

\node[initial, state] (R) {z\textsubscript{0}};
\node[state] (S) [right of=R] {z\textsubscript{1}};
\node[state, accepting] (T) [right of=S] {z\textsubscript{2}};

\path (R) edge [below] node {1}(S)
	  (S) edge [loop, above] node{0} (S)
	  (S) edge [below] node {1} (T)
	  ;
\end{tikzpicture}
\caption{Finite automaton}
\label{fig:FiniteAutomaton}
\end{figure}

\subsection{Regular expression}\label{sec:BackgroundRegEx}

A regular expression is an algebraic description of a regular language.
Regular expressions declare strings that are part of the language and can describe the same words that can also be represented by a finite automaton. 
In comparison to finite automata, the words can be described in an algebraic way. 
Due to the similarities between regular expressions and finite automata, it is possible to convert regular expressions to finite automata and backwards. 
Regular expressions are often used to describe tokens that should be recognized by a lexer (see section \ref{sec:BackgroundLexer}). \cite{AutomataTheory.2007}\\
For example the regular expression \textbf{10*} denotes the language consisting of words that are made up a single 1 followed by an arbitrary number of 0's.\\
Given an alphabet $\sum$, regular expressions are formally defined as followed \cite{AutomataTheory.2007}:
\begin{enumerate}
\item The constants $\epsilon$ and $\emptyset$ are regular expressions denoting the empty string and the empty set.
\item If $a$ is a symbol of the alphabet $\sum$, then $a$ is a regular expression.
\item If $a$ and $b$ are regular expressions, then the alternation $a|b$ (either $a$ or $b$), the concatenation $ab$ ($a$ followed by $b$) and the Kleene star $a*$ (an arbitrary number of $a$) are regular expressions.
\end{enumerate}


\subsection{Formal grammars}\label{sec:BackgroundGrammar}

Formal grammars describe how words of a language can be generated. In comparison to regular expressions, more classes of languages can be described. While regular expressions only describe regular grammars, formal grammars are able to describe all types of grammars of the Chomsky hierarchy.
%Grammars process data using a recursive structure.

%-s Ok, aber for allem können sie viel größer Klassen von Sprachen beschreiben!
%-...und wenn ich schon dabei bin: Machen REs das nicht auch? Nur halt anders.

The description of a grammar consists of four components:

\begin{itemize}

\item A set of symbols that defines the alphabet of the language. These symbols are called terminal symbols. The words of a language only consist of terminal symbols.

\item A set of variables called nonterminal symbols. Each nonterminal symbol represents a set of words. 

\item One nonterminal symbol represents the language that is being defined. This nonterminal symbol is called the start symbol. Other nonterminal symbols help to define the language of the start symbol.

\item A set of productions/rules. They recursively describe the language. Each production consists of a nonterminal symbol on the left-hand side that is being substituted by a sequence of terminal symbols and/or other nonterminal symbols on the right-hand side. There can also be empty productions, meaning a nonterminal symbol is substituted by an empty string.
\end{itemize}

%- ambigious grammars
%- precedence rules?

\cite{AutomataTheory.2007}

%A grammar is a list of rules that defines the relationships between tokens \cite{LexYacc.1992}.
%These rules are also referred to as production rules.
%Given a start symbol, this symbol can be replaced by other symbols using the production rules.
%Using a recursive notation, production rules define derivations for words. The derived symbols can then once again be replaced until the derivation
%
%s Hier sauber trennen:
%- Die Ableitung ist eine Folge von Regelanwendungen
%- Das abgeleitete Wort
%- Terminale und nicht-terminale Symbole
%Speziell: Das abgeleitete Wort ist im allgemeinen kein Symbol
%(weder terminal noch nicht-terminal), sondern es besteht aus
%Symbolen (und wenn die Ableitung fertig ist, nur aus Terminalsymbolen
%
%is a terminal symbol.  
%Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language is built by the set of terminal symbols.
%Nonterminal symbols however can be further derived and todo ?build  merged? with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 
%\\
%- Beispiel

\subsubsection{Context-free grammars}

A context-free grammar is a grammar that only allows productions rules that are in the form of $V \rightarrow \beta$ with V being a nonterminal symbol and $\beta$ being a sequence of nonterminal and terminal symbol with an arbitrary length.
Context-free grammars are often the basis of a parser (see section \ref{sec:BackgroundParser}). \cite{AutomataTheory.2007}

\subsubsection{Reduced grammars}

Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}.\\
Given the set of terminal symbols $\sum$, a nonterminal symbol $A$ is called terminating if there are productions $A \underrightarrow{*} z$ (meaning some sequence of productions can be applied) so that $z$ can be derived from $A$ in one or multiple steps and $z \epsilon \sum$*.\\
In other words, a nonterminal symbol $A$ is terminating if there exist production rules so that  $A$ can be replaced by a string of terminal symbols. \cite{Cremers75}\\
Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $A$ is called reachable if there are production rules $S \underrightarrow{*} uAv$ so that $S$ can be derived to $uAv$ and $u,v \epsilon \sum$*.\\
In other words, a nonterminal symbol $A$ is reachable if there exist production rules so that the start symbol can be replaced by a word containing $A$. \cite{Cremers75}

%todo beispiel

\section{\acf{BNF}}\label{sec:BackgroundBNF}

The \acf{BNF} is a language to describe context-free grammars.
In the \acf{BNF} nonterminal symbols are distinguished from terminal symbols by being enclosed by  angle brackets, e. g. <$TPTP\_File$> denotes the nonterminal symbol $TPTP\_File$.
Productions are described using the $"::="$ symbol and alternatives are specified using the $"|"$ symbol. \cite{BNF.1964} 
Alternative symbols are used to conveniently describe several productions with the same left-hand side nonterminal symbol.
An example for a \ac{BNF} production using an alternative symbol would be 
\begin{verbatim}
<TPTP_File> ::= <TPTP_Input> | <comment>
\end{verbatim}
Without using alternative symbols it would be
\begin{verbatim}
<TPTP_File> ::= <TPTP_Input>
<TPTP_File> ::= <comment>
\end{verbatim}
which is longer than the previous notation. 
Using the \ac{BNF} pattern of notation whole grammars can be specified.\\
The \ac{EBNF} extends the \ac{BNF} by with following rules:

\begin{itemize}%[noitemsep]
	\item Optional expressions are surrounded by square brackets.
	\item Repetition is denoted by curly brackets.
	\item Parentheses are used for grouping.
	\item Terminals are enclosed in quotation marks.
\end{itemize}
\label{itemize:BackgroundBNF}
\cite{EBNF.1977}

\section{TPTP language}\label{sec:BackgroundTPTP}

The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its syntax is specified in an \ac{EBNF}. \cite{Sut17}

Originally the \ac{TPTP} used only \ac{CNF} \cite{Sut09}, but over the
years expanded to various other types of logics including
include \ac{FOF} \cite{Sut09}, Typed \ac{FOF} \cite{SS+12, BP:CADE-2013} and \ac{THF} \cite{SB10, KSR:PAAR-2016}.

The TPTP syntax uses an extended \ac{BNF}, with distinct rules for syntax, semantic constraints, tokens, and character classes which will be described in more detail in section \ref{sec:ConceptLexer}.
A small tool chain based on Lex/Yacc for building a basic parser from the syntax is available \cite{VS06}.
While the TPTP is a major success story, the resulting syntax is large and complex.
Understanding the syntax is non-trivial.
For new developers who want to implement ``only'' a first theorem prover for \ac{CNF}, identifying the relevant parts of the syntax and implementing a parser is a significant barrier to entry.


\section{Lexing}\label{sec:BackgroundLexer}

Lexing or a so-called lexical analysis is the division of input into units called tokens~\cite{LexYacc.1992}.\\
The input is a string containing a sequence of characters.
The lexer groups characters of the input string into sequences that are meaningful. These sequences are called lexemes. From each lexeme, the lexer produces a token, which consists of the token name and the lexeme string. The token name is an abstract symbol which is used during parsing. \cite{Aho.2007} \\
The tokens are then passed to the parser for syntax analysis.\\
A lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input \cite{Mogensen.2017}.\\
A simple approach to build a lexer would be building an automaton for each token definition and then test to which automata the input corresponds.\\
However, this would be inefficient because in the worst case the input needs to pass all automata before the belonging automata is identified.
Vuilding a single automaton that tests each token simultaneously is more suitable.
This automaton can be built by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.\\
Potentially final states overlap as a consequence of one token being a substring of another token. \\
For solving such conflicts, a lexer is separating the input in order to divide it into tokens.
Per convention the lexer chooses the longest input that matches any token.~\cite{Mogensen.2017} \\
Furthermore, a precedence of tokens can be declared. Usually the token that is being defined first has a higher precedence and thus will be chosen if possible token matches have the same length. \cite{Mogensen.2017}

Besides of writing a lexer manually it can also be generated by a lexer generator.
A lexer generator takes a specification of tokens as input and generates the lexer automatically. 
The specification of the tokens is usually written using regular expressions. 

\section{Parsing}\label{sec:BackgroundParser}

Parsing is the process of analysing the grammatical structure of a sequence of tokens and can be used to verify that the input of the parser is part of the language that the parser accepts \cite{Aho.2007}.
To do so, a parser builds a parse tree out of the tokens~\cite{Mogensen.2017}.\\
Parsers can also be generated automatically.
A parser generator takes a description of the relationship among tokens in form of a formal grammar (see section \ref{sec:BackgroundGrammar}) as input. The output is the generated parser. \cite{LexYacc.1992}\\
During the syntax analysis a parser takes a string of tokens and forms a syntax tree by finding the matching derivations. The matching derivation can be found using various approaches. The approaches that are used in this report will be introduced in the following.

\subsection{Bottom-up parsing}\label{sec:BackgroundParserBottomUp}

In bottom-up parsing a parse tree for an input string is constructed beginning at the leaves working up to the root. The idea is to reduce a string to the start symbol of a grammar, constructing a deviation in reverse.
\cite{Aho.2007}\\
At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that productions (reduction is reverse step of deviation).
The key decisions are when to reduce and what production to apply.
These decisions depend on the parser implementation.
An example for bottom-up are shift-reduce parsers that are described in the following.
\cite{Aho.2007}
\subsection{Shift-reduce parsing}\label{sec:BackgroundParserShiftReduce}

Shift-reduce parsers are a class of bottom-up parsers. In shift-reduce parsing a stack is used to hold grammar symbols and an input buffer holds the rest of the input string.
During parsing the input is scanned from left to right. The parser shifts zero or more input symbols on the stack until it can reduce the elements at the top of the stack. The elements at the top of the stack are reduced to the head of the corresponding production. This procedure is repeated until the stack only contains the start symbol and the input buffer is empty or an error is detected. \cite{Aho.2007}\\
There are four actions a shift-reduce parser can perform:
\begin{itemize}%[noitemsep]
	\item \textbf{Shift}: Shift next input symbol on top of stack.
	\item \textbf{Reduce}: With the right end of a string to be reduced being at top of the stack and the left end of the string located within the stack, the string is replaced with a  chosen nonterminal.
	\item \textbf{Accept}: Declare that input was successfully parsed.
	\item \textbf{Error}: Detect a syntax error.
\end{itemize}
\cite{Aho.2007}

%-use justified by, handle will appear on top of stack not inside, proof ...

There are two kinds of conflicts that can occur during shift-reduce parsing. A shift-reduce conflict occurs if the parser cannot decide whether to shift or reduce.
A reduce-reduce conflict occurs if the parser cannot decide which of multiple reductions to make.\cite{Aho.2007}

%-these grammars are not in LR(k) class of grammars (non-LR grammars)
%for example: ambiguous grammar can never be LR (if else)
%\cite{Aho.2007}

\subsection{LR($k$) parsing}\label{sec:BackgroundParserLR}

LR($k$) parsing is the most common type of bottom-up parsing. The L stands for left-to-right, meaning the input is read from left to right, and R stands for constructing a rightmost derivation in reverse.
In the context of LR parsing, the rightmost derivation refers to reducing the input to the start symbol.
The parameter $k$ refers to the number of inputs symbols that are used as a lookahead. When $k$ is omitted, $k = 1$ is assumed.
An LR parser makes shift-reduce decisions based on transitioning to different parsing states.
An LR parser consists of a stack and a parsing table defining parsing-action functions and goto functions.
An action function takes a state and an input symbol as input and defines which action to perform (Shift, Reduce, Accept, Error).
A goto function takes a state and a nonterminal symbol as input and is used to find the next state after a reduction.
States represent set of items.
An item is a production of the grammar that the parser is based on with a dot at some position on the right-hand side of the production.
An item indicates how much of a right-hand side has already been seen and what the parser is expecting next to apply the production (lookahead).
For example $A \rightarrow B\cdot C$ indicates that the parsers has already read the input $B$ and expects $C$ in order to reduce the right-hand side to $A$.\cite{Aho.2007}\\
LR parsers are complex to implement, but LR parser generators can be used, which significantly reduces the effort for creating an LR parser.
\cite{Aho.2007}

\subsubsection{LALR parsing}\label{sec:BackgroundParserLALR}

An LALR parser (Look-Ahead LR parser) is a simplified LR parser.
The simplification consists of merging items that have identical cores.
A core is the part of the right-hand side of a productions that is left of the dot. 
For example an LR parser would separate the items $A \rightarrow B\cdot C$ and $A \rightarrow B\cdot D$ as their core is identical but their lookahead is different.
An LALR parser merges the two items to one item
$A \rightarrow B\cdot \{C,D\}$.
Not considering the lookahead reduces the power of the parser because it might lead to reduce/reduce conflicts.
LALR parsers are nevertheless often used because the parsing table is much smaller comparing to other LR parsing techniques  due to merging items.
\cite{Aho.2007}

\section{Lex and Yacc}\label{sec:BackgroundLexYacc}

Lex and Yacc are tools for writing lexers and parsers. They often work together, meaning the parser uses the generated tokens from the lexer as input.

\subsection{Lex}\label{sec:BackgroundLex}

Lex is a tool written in C that generates a lexer by specifying regular expressions and corresponding code fragments. The specified regular expressions are translated into a program that reads an input stream. The program partitions the input stream into strings matching the regular expressions. Once an expression is recognized the corresponding code is executed. For recognizing expressions, Lex builds a deterministic finite automaton. The automaton chooses the longest possible match if the input fits multiple specified regular expressions. \cite{Lex}\\
Flex is an open-source alternative to Lex and also compatible to Lex. \cite{Flex}

\subsection{Yacc}\label{sec:BackgroundYacc}
Yacc is a tool written in C that can be used for parsing the input of a computer program. 
For generating a parser using Yacc, a specification that specifies the structure of the input has to be given. In addition to the specification of the structure, the input to the parser generator also consists of code that is invoked once a structure has been recognized. 
Yacc turns the input specification into a routine that handles the input. This routine calls Lex to get tokens from the input stream and arranges them based on the specification. \cite{Yacc}\\
Bison is an open-source general-purpose parser generator that can be used as an alternative to Yacc and also accepts Yacc grammars. \cite{Bison}

\section{Python}\label{sec:BackgroundPython}

Since the use cases of \ac{Synplifier} are all hand-created grammars of relatively small size (dozens to thousands of rules), and the algorithms do not have high complexity, we decided to implement the tool in Python 3. 
Python is a simple but powerful modern multi-paradigm language with good support and excellent libraries.
Python is easy to learn and its power allows it to create complex applications using Python.
Libraries that are used in the implementation \ac{Synplifier} are introduced in the following sections.

\subsection{PLY}\label{sec:BackgroundPythonPLY}

\acf{PLY} is an implementation of lex and yacc in Python. It is compatible with Python 2 and 3 and is fully implemented in Python. The goal of \ac{PLY} is to offer a pure-Python implementation of lex and yacc. The implementation aims to rebuild the functionalities of lex and yacc and thus includes:
\begin{itemize}
\item Support for LALR parsing (see chapter \ref{sec:BackgroundParserLALR})
\item Support for empty productions
\item Support for ambiguous grammars 
\item Precedence rules
\item Error checking and recovery
\end{itemize}

\ac{PLY} consists of the two modules $lex.py$ and $yacc.py$ that are meant be to execute together. I. e. $yacc.py$ retrieves tokens from the input stream provided by $lex.py$. $Yacc.py$ returns an Abstract Syntax Tree by default. However, it is possible for the user to change the output of $yacc.py$ according to his demands. \cite{PLY}
PLY is used to generate the lexer and parser for the \ac{TPTP} syntax files.

\subsection{PyQt}\label{sec:BackgroundPytonPyQt}

PyQt is a Python binding for the cross-platform GUI framework Qt.
It is licensed under the GNU GPL version 3.
Qt offers a set of C++ libraries and development tools for various things including tools for building GUIs, defining regular expressions, setting up SQL databases, using NFC or interacting via Bluetooth.
PyQt implements over 1000 of these Qt tools in Python.
Using PyQt a GUI consists of a main window (QMainWindow).
A simple GUI can be created by subclassing the QMainWindow, which provides a main window for an application
The QTreeView class provides an implementation of a tree view, which can be used to display data in a hierarchically structured way. \cite{PyQt}

\subsection{argparse}\label{sec:BackgroundArgparse}

The Python module argparse is a module for creating command-line argument parsers.
It provides the means to specify input arguments and automatically creates help and usage messages.
It also checks if the given arguments are valid.
From the specified input arguments, the module will automatically create a parser for the specified arguments. \cite{argparse}\\
This module is used in the implementation of the command-line interface of \ac{Synplifier}.

\subsection{Beautiful Soup}\label{sec:BackgroundBeautifulSoup}

Beautiful Soup is a Python library for extracting data out of HTML and XML files.
That makes it especially useful for getting information from web pages. \cite{BeautifulSoup,BeautifulSoupDoku}
In \ac{Synplifier} Beautiful soup is used to automatically extract the latest \ac{TPTP} syntax from the HTML version on the \ac{TPTP} website.
