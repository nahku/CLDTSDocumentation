%!TEX root = ../dokumentation.tex

\chapter{Background and Theory}\label{cha:Background}

This chapter introduces the technologies and background that will be utilized in the following chapters. First, an introduction into the \ac{TPTP} language is given. Then, formal grammars and the \ac{BNF} are described. Following that, the foundations of lexing and parsing are outlined. Finally, Python and relevant Python modules that are used in the implementation are presented.

\section{TPTP language}\label{sec:BackgroundTPTP}

The \acf{TPTP} is a library of problems for \ac{ATP}.
Problems within the library are described in the \ac{TPTP} language.
The  \ac{TPTP} language is a formal language and its syntax is specified in an \ac{EBNF}. \cite{Sut17}\\

TODO more detailed

\section{Formal languages}\label{sec:BackgroundFormalLanguage}

A formal language is a set of words over an alphabet.

An alphabet is a finite, non-empty set of symbols usually represented by $\sum$. An example is the binary alphabet $\sum = \{0,1\}$.
A string is a finite sequence of symbols from some alphabet. For example the string $101$ is a string over the binary alphabet $\sum = \{0,1\}$.
A language is set of strings. If $\sum$ is an alphabet, then    $L(\sum)$ is a language over $\sum$. \cite{AutomataTheory.2007}

- Vocabulary
s ? Welche? Oder soll es die Menge aller solcher Sprachen sein? Dann nicht "a language".

\subsection{Finite automata}\label{sec:BackgroundAutomata}

\subsection{Regular expression}\label{sec:BackgroundRegEx}

A regular expression is an algebraic description of a regular/formal language. Regular expressions declare strings that are part of the language. \cite{AutomataTheory.2007} 
For example the regular expression \textbf{10+1*} denotes the language consisting of a single 1 followed by a single 0 or any number of 1's. 

\subsection{Formal grammars}\label{sec:BackgroundGrammar}

Unlike regular expressions, grammars not only describe a language but also define a structure of the words of a language.\\

-s Ok, aber for allem können sie viel größer Klassen von Sprachen beschreiben!
-...und wenn ich schon dabei bin: Machen REs das nicht auch? Nur halt anders.

A grammar is a list of rules that defines the relationships between tokens \cite{LexYacc.1992}.
These rules are also referred to as production rules.
Given a start symbol, this symbol can be replaced by other symbols using the production rules.
Using a recursive notation, production rules define derivations for words. The derived symbols can then once again be replaced until the derivation

s Hier sauber trennen:
- Die Ableitung ist eine Folge von Regelanwendungen
- Das abgeleitete Wort
- Terminale und nicht-terminale Symbole
Speziell: Das abgeleitete Wort ist im allgemeinen kein Symbol
(weder terminal noch nicht-terminal), sondern es besteht aus
Symbolen (und wenn die Ableitung fertig ist, nur aus Terminalsymbolen

is a terminal symbol.  
Terminal symbols describe symbols that cannot be further derived. The alphabet of the described language is built by the set of terminal symbols.
Nonterminal symbols however can be further derived and todo ?build  merged? with the terminal symbols the vocabulary of a grammar. Nonterminal symbols and terminal symbols are disjoint. 
\\
- Beispiel

\subsubsection{Context-free grammars}

-language is set of strings
-recursive notation

-description of language contains four components

1. set of symbols that defines strings of the language, called terminal symbols

2. set of variables, called nonterminal symbols. Each symbol represents a set of strings

3. one nonterminal symbol represents the language that is being defined. This nonterminal symbol is called the start symbol

4. set of productions/rules. They represent the recursive definition of the language. Each production consists of: nonterminal symbol that is being (partially) defined in production, production symbol, and a string of terminal symbols and/or variables.
In production terminals are unchanged and nonterminal symbols are substituted by its production body

Derivations:

\cite{AutomataTheory.2007}

\subsubsection{Reduced grammars}

Grammars are called reduced if each nonterminal symbol is terminating and reachable \cite{Cremers75}. \\
Given the set of terminal symbols $\sum$, a nonterminal symbol $\xi$ is called terminating if there are productions $\xi \underrightarrow{*} z$ so that $z$ can be derived from $\xi$ and $z \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is terminating if there exist production rules so that  $\xi$ can be replaced by a string of terminal symbols. \cite{Cremers75}\\
Given the set of terminal symbols $\sum$ and the start symbol $S$, a nonterminal symbol $\xi$ is called reachable if there are production rules $S \underrightarrow{*} u\xi v$ so that $S$ can be derived to $u\xi v$ and $u,v \epsilon \sum$*. \\
In other words, a nonterminal symbol $\xi$ is reachable if there exist production rules so that the start symbol can be replaced by a word containing $\xi$. \cite{Cremers75}

todo beispiel

\section{\acf{BNF}}\label{sec:BackgroundBNF}

The \acf{BNF} is a language to describe context-free grammars.
In the \acf{BNF} nonterminal symbols are distinguished from terminal symbols by being enclosed by  angle brackets, e. g. <$TPTP\_File$> denotes the nonterminal symbol $TPTP\_File$.
Productions are described using the $"::="$ symbol and alternatives are specified using the $"|"$ symbol. \cite{BNF.1964}
An example for a \ac{BNF} production would be $TPTP\_File$ $::=$ <$TPTP\_Input$> $|$ <$comment$>.
Using this pattern of notation whole grammars can be specified.\\
The \ac{EBNF} extends the \ac{BNF} by with following rules:

\begin{itemize}%[noitemsep]
	\item optional expressions are surrounded by square brackets.
	\item repetition is denoted by curly brackets.
	\item parentheses are used for grouping.
	\item terminals are enclosed in quotation marks.
\end{itemize}
\label{itemize:BackgroundBNF}
\cite{EBNF.1977}

\section{Lexing}\label{sec:BackgroundLexer}

Lexing or a so-called lexical analysis is the division of input into units called tokens \cite{LexYacc.1992}.\\
The input is a string containing a sequence of characters.
The lexer groups characters of the input string into sequences that are meaningful. These sequences are called lexemes. From each lexeme, the lexer produces a token, which consists of the token name and the lexeme string. The token name is an abstract symbol which is used during parsing. \cite{Aho.2007}

-todo example

The tokens are then passed to the parser for syntax analysis.\\
A lexer needs to distinguish different types of tokens and furthermore decide which token to use if there are multiple ones that fit the input \cite{Mogensen.2017}.\\
A simple approach to build a lexer would be building an automaton for each token definition and then test to which automata the input corresponds.\\
However, this would be inefficient because in the worst case the input needs to pass all automata before the belonging automata is identified.
More suitable is building a single automaton that tests each token simultaneously.
This automaton can be built by combining all regular expressions by disjunction.
Each final state from each regular expression is marked to know which token has been identified.\\
Potentially final states overlap as a consequence of one token being a substring of another token. \\
For solving such conflicts, a lexer is separating the input in order to divide it into tokens.
Per convention the lexer chooses the longest input that matches any token. \cite{Mogensen.2017} \\
Furthermore, a precedence of tokens can be declared. Usually the token that is being defined first has a higher precedence and thus will be chosen if possible token matches have the same length. \cite{Mogensen.2017}

Besides of writing a lexer manually it can also be generated by a lexer generator. A lexer generator takes a specification of tokens as input and generates the lexer automatically. 
The specification is usually written using regular expressions. 

\section{Parsing}\label{sec:BackgroundParser}

The aim of parsing is to establish a relationship among tokens generated by a lexer \cite{LexYacc.1992}. For doing so, a parser builds a parse tree out of the generated tokens \cite{Mogensen.2017}.\\
Similar to lexers, parsers can be generated automatically.
A parser generator takes as input a description of the relationship among tokens in form of a formal grammar (see ). The output is the generated parser. \cite{LexYacc.1992}\\
During the syntax analysis a parser takes a string of tokens and forms a syntax tree with this construct by finding the matching derivations. The matching derivation can be found by using different approaches for example random guessing (predictive parsing) or LR parsing.
Input: description of grammar \cite{LexYacc.1992}
Output: parser \cite{LexYacc.1992}

\subsection{Bottom-up parsing}\label{sec:BackgroundParserBottomUp}

In bottom-up parsing a parse tree for an input string is constructed beginning at the leaves working up to the root. The idea is to reduce a string to the start symbol of a grammar, constructing a deviation in reverse.
\cite{Aho.2007}

-at each reduction step specific substring matching body of a production is replaced by the nonterminal at the head of that productions (reduction is reverse step of deviation)
-key decisions are when to reduce and what production to apply

\subsubsection{Shift-reduce parsing}\label{sec:BackgroundParserShiftReduce}

Shift-reduce parsers are one class of bottom-up parsers. In shift-reduce parsing a stack is used to hold grammar symbols and an input buffer holds the rest of the input string.
During parsing the input is scanned from left to right. The parser shifts zero or more input symbols on the stack until it can reduce the elements at the top of the stack. The elements at the top of the stack are reduced to the head of the corresponding production. This procedure is repeated until the stack contains the start symbol and the input buffer is empty or an error is detected. \cite{Aho.2007}\\
There are four actions a shift-reduce parser can perform:
\begin{itemize}%[noitemsep]
	\item \textbf{Shift}: Shift next input symbol on top of stack.
	\item \textbf{Reduce}: todo right end of sting to be reduced must be at top of stack, locate left end of string within the stack and decide with what nonterminal to replace the string.
	\item \textbf{Accept}: Declare that Input was successfully parsed.
	\item \textbf{Error}: Detect a syntax error.
\end{itemize}
\cite{Aho.2007}

-use justified by, handle will appear on top of stack not inside, proof ...

There are two kinds of conflicts that can occur during shift-reduce parsing. A shift-reduce conflict occurs if the parser cannot decide whether to shift or reduce.
A reduce-reduce conflict occurs if the parser cannot decide which of multiple reductions to make.

-these grammars are not in LR(k) class of grammars (non-LR grammars)
for example: ambiguous grammar can never be LR (if else)
\cite{Aho.2007}

\subsubsection{LR($k$) parsing}\label{sec:BackgroundParserLR}

LR($k$) parsing most common type of bottom-up parser. The L stands for left-to-right and R stands for constructing a rightmost derivation in reverse. The parameter $k$ refers to the number of inputs symbols that are used as a lookahead. When $k$ is omitted, $k = 1$ is assumed.
LR-Parsers are complex to implement, but LR parser generators can be used, which significantly reduce the effort in creating an LR-Parser

-what is rightmost derivation
continue at s 241


\cite{Aho.2007}
-lr p. 241

\subsubsection{LALR parsing}\label{sec:BackgroundParserLALR}

\cite{Aho.2007}
-p.259

\section{Lex and Yacc}\label{sec:BackgroundLexYacc}

Lex
-specify lexer by regular expressions
-Lex compiler generates code from input in file lex.yy.c
(p.140)
Yacc
-transforms yacc specification to c file y.tab.c
-y.tab.c represens LALR parser
(p. 287)
\cite{Aho.2007}

-automated parser generator creates lex and yacc specifications and generated files from \ac{TPTP} syntax

\section{Python}\label{sec:BackgroundPython}

Since the use cases are all hand-created grammars of relatively small
size (dozens to thousands of rules), and the algorithms do not have
high complexity, we decided to implement the tool in Python 3. Python
is a modern multi-paradigm language with good support and excellent
libraries.

\subsection{PLY}\label{sec:BackgroundPythonPLY}

\acf{PLY} \cite{PLY} is an implementation of lex and yacc in python.
[LALR-parsing] \ref{sec:BackgroundParserLALR}
consists of lex.py and yacc.py

lex.py tokenizes an input string

\subsection{PyQt}\label{sec:BackgroundPytonPyQt}

PyQt is a Python binding for the cross-platform GUI framework Qt \cite{PyQt}.
It is licensed under the GNU GPL version 3.
- QMainWindow \\
- QTreeWidget \\
-pyqt 5

tkinter

\subsection{argparse}\label{sec:BackgroundArgparse}

The python module argparse \cite{argparse} is a module for creating command-line argument parsers.
It provides the means to specify input arguments and automatically creates help and usage messages.
It also checks if the given arguments are valid.
From the specified input arguments, the module will automatically create a parser for the specified arguments.