 %!TEX root = ../dokumentation.tex

%TODO: Einleitungen überarbeiten
\chapter{Concept}\label{cha:Concept}
This chapter outlines the concept and the architecture of the software tool.
First, in section \ref{sec:ConceptRequirements}, the requirements the software tool needs to meet are described.
Then, in section \ref{sec:ConceptOverview}, the components needed are introduced.
Then the proposed software architecture is described. After that the concept of each component is developed.

\section{Requirements}\label{sec:ConceptRequirements}
The tool should meet the following requirements:\\
The tool has a GUI that is the interface between the tool and a user. Hence, the user communicates with the tool via the GUI. The user is able to import a syntax file. After the syntax file is imported, it should is displayed. 
This includes displaying by the syntax defined productions as well as comments that are associated with these productions.
The user can select a new start symbol and can select which productions should be blocked. 
After the user made his choice, the new sub-syntax is generated and displayed.
The tool can also generate a control file listing blocked productions and the start symbol.
Furthermore, the tool is able to import a control file and extract a sub-syntax based on this control file instead of extracting a sub-syntax based on a users selection of blocked productions.
The new sub-syntax can be exported to .txt format.
Also, comments referring to the remaining productions are kept and comments referring to productions that were discarded are not be included in the sub-syntax.
The tool also provides a console interface. This interface accepts a \ac{TPTP} syntax file and a control file and output the sub-syntax described in the control file. It is possible to specify the output path and filename.
\section{Overview}\label{sec:ConceptOverview}

Figure \ref{fig:ConceptProcessSublanguage} outlines the procedure of extracting a sublanguage of the \ac{TPTP} language.
The first task is to import the \ac{TPTP} syntax file and extract the tokens inside that file using the lexer.
The next phase is for the parser to create a data structure from the tokens, also checking if the syntax in the syntax file was correct.
Then, a graph representing the imported \ac{TPTP} syntax should be built.\\
This graph is subject to manipulation by disabling certain transitions or selecting a new start symbol in the following phase.
This includes computation of the remaining reachable and terminating grammar.
That new graph represents the syntax of the extracted sub-language.
To make this grammar usable, lastly the syntax has to be output, based on the new graph, in the same format as the original syntax.
\begin{figure}[H]
\tikzstyle{decision} = [ diamond, draw, fill=blue!10, text width=4.5em, text badly centered, node distance=2cm, inner sep-0pt]  
\tikzstyle{block} = [ rectangle, draw, fill=blue!10, text width=4.5em, text badly centered, rounded corners, minimum height=4em]  
\tikzstyle{line} = [ draw, -latex']  
\tikzstyle{terminator} = [rectangle, draw, fill=blue!10, text width=4.5em, text badly centered, rounded corners, minimum height=4em]  
\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]  
  \node [terminator]  (lex)  {Import of syntax file and lexing};  
  \node [block, right of=lex]  (pars) {Parsing};  
  \node [block, right of=pars] (ggg) {Grammar graph generation}; 
  \node [block, right of=ggg] (ggm) {Grammar graph modification}; 
   \node [block, right of=ggm] (go) {Grammar output};  
  \path [line] (lex)  -- (pars);  
  \path [line] (pars) -- (ggg);  
  \path [line] (ggg) -- (ggm); 
  \path [line] (ggm) -- (go);  
\end{tikzpicture}
\end{center}
\caption{Procedure of extracting a sublanguage}
\label{fig:ConceptProcessSublanguage}
\end{figure}

\subsection{Proposed architecture}\label{sec:ConceptProposedArchitecture}
The architecture of the software tool should take the procedure of extracting a sublanguage (section \ref{sec:ConceptOverview}) into consideration.
From that, five main components can be identified:
An import module responsible for importing the \ac{TPTP} syntax from a file;
A lexer for extracting tokens from the language specification; A parser for creating a data structure from the tokens;
A graph builder and manipulator;
An export module for exporting the graph in a text representation corresponding to the original language specification.\\
In addition to the components that provide the main functionality a graphical user interface and a console interface for user convenience is desired.

Figure \ref{fig:ConceptArchitectureOverview} contains a high-level UML diagram describing the architecture of the software tool. The user interacts either with the \textit{Console} or \textit{View} class. The \textit{Console} class provides the command-line interface and the \textit{View} class provides the GUI. Both have a reference on \textit{Input} and \textit{Output} for reading from and writing to files. They also have a reference on the \textit{TPTPGraphBuilder} class. This class is responsible for building a grammar graph and extracting sub-syntaxes by graph manipulation. For that, lexing and parsing are necessary. The \textit{TPTPGraphBuilder} uses the \textit{Parser} class for getting a \ac{TPTP} syntax representation and the \textit{Parser} uses the \textit{Lexer} to extract the tokens from a \ac{TPTP} syntax file.
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/Concept_UML_Architecture_Overview.pdf}
\caption{UML diagram of the architecture of the software tool}
\label{fig:ConceptArchitectureOverview}
\end{figure}
\subsection{Implementation language}\label{sec:ConceptImplementationLanguage}
todo why python

\section{Lexer}\label{sec:ConceptLexer}
The lexer is responsible for extracting tokens from the TPTP language grammar specification file. Using \ac{PLY} a lexer can be built by specifying tokens as regular expressions.\\
Therefore the TPTP language grammar specification needs to be analysed in order to find elementary tokens and regular expressions, that precisely describe these
tokens. 

\subsection{Token identification}\label{sec:ConceptElementaryTokens}
The syntax of the \ac{TPTP} language is specified in a modified \ac{EBNF} \cite{VS06}.
Therefore there are deviations from standard \ac{EBNF} (see \ref{sec:BackgroundBNF}) that need to be analysed to specify elementary tokens.
The standard \ac{EBNF} only uses one production symbol ($"::="$).
In the \ac{TPTP} syntax additional production symbols have been added.
The following table \ref{tbl:ConceptTPTPProductionSymbols} contains the production symbols used in the \ac{TPTP} syntax, that also have to be recognized by the lexer.
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1}
\caption{\ac{TPTP} language production symbols \cite{VS06}}
\begin{tabular}{ll}
\textbf{Symbol} & \textbf{Rule Type}\\\hline
::= & Grammar\\
:== & Strict\\
::- & Token\\
::: & Macro\\
\end{tabular}
\label{tbl:ConceptTPTPProductionSymbols}
\end{table}

Another deviation from \ac{EBNF} is that repetition is not denoted by surrounding curly brackets, but with a trailing $*$ symbol.\\
Curly brackets have no special meaning in the \ac{TPTP} syntax and can be treated as terminal symbols.\\
The meaning of the alternative symbol $|$ is unchanged and also parentheses and square brackets can appear as meta symbols.\\
Also, there are line comments in the \ac{TPTP} syntax.
A comment starts with the $\%$ symbol at the beginning of a line and ends at the end of that line.\\
Following standard \ac {BNF}, nonterminal symbols are enclosed by the $<$ and $>$ symbol and terminal symbols are written without any special marking.

%todo regular expressions
 
%todo explain token nt symbol + production rule bc. of parser

\subsection{Tokens}\label{sec:ConceptTokens}

This section introduces the token, their regular expression are described in chapter \ref{sec:ImplementationLexer}.

It is possible to declare symbols that should be ignored. However, if a symbol is declared as ignored but is specifically mentioned in another token,
then if the sequence of characters represent that token, the ignored symbol is not ignored. In this project, tabs and white spaces are ignored as they do not have any special meaning other than providing clarity. Also, newlines are generally ignored because as can be seen in listing \ref{lst:Lexer_example_multiline} there are rules that cover multiple lines. 

\begin{lstlisting}[basicstyle=\scriptsize	,caption= Example of a multi line production rule,label= lst:Lexer_example_multiline]
<annotated_formula>    ::= <thf_annotated> | <tff_annotated> | <tcf_annotated> |
                           <fof_annotated> | <cnf_annotated> | <tpi_annotated>
\end{lstlisting}

Apart from the ignored symbols, there are 13 defined tokens.
\subsubsection{Expression}
\textit{Expressions} can either be of the type grammar, token, strict or macro. 
It is defined as a nonterminal symbol followed by the production symbol itself (::=,:==,:::,...). 
The nonterminal symbol and the production are merged to a single token and are not identified as two
tokens to avoid ambiguity while parsing.
If not it would be difficult for the parer to determine whether the non terminal symbol that describes the rule is the start of a new rule or does still belong to the previous rule because as mentioned rules can cover multiple lines.

\subsubsection{Nonterminal symbol}
A \textit{nonterminal} symbol starts with \textless and ends with \textgreater . In between there is any
arbitrary sequence of numbers, underscores and small or capital letters.

\subsubsection{Terminal symbol}
\textit{terminal symbol}

\subsubsection{Comment}
A \textit{comment} is identified by the lexer as a start of a new line followed by a percentage
sign followed by an arbitrary character and ends with a newline.
Because the
percentage sign is also part of the terminal symbols, it is necessary to check whether
the percentage sign is the first character of a newline because the terminal symbol is not because the
percentage symbol when used as terminal symbol is embedded in square brackets.

\subsubsection{Meta symbol}
\textit{Meta symbols} include open and close parentheses "( )", open and close square
brackets "[ ]", asterisks "*"and vertical bars "|".
They are recognized by the symbol itself and have a special meaning for the parser as they impact the to be build data structures.

\section{Parser}\label{sec:ConceptParser}
The parser takes the tokens from the lexer as input and creates a data structure that represents the structure of the \ac{TPTP} syntax.

Figure \ref{fig:ConceptParserFlow} outlines the responsibilities of the parser component and the sequence of its sub-functions.
First, the tokens generated by the lexer need to be parsed and based on that the data structure representing the \ac{TPTP} syntax is to be created.
The rules in the data structure have to be numbered, to maintain the correct order for output, after creating the grammar tree in the next step (see section \ref{sec:Concept}).

In the \ac{TPTP} syntax square brackets not necessarily denote that an expression is optional.
In token and macro rules they have the same meaning as in traditional \ac{EBNF} and in grammar and strict rules square brackets are terminals.
Therefore disambiguation of square brackets is necessary.
\begin{figure}[H]
\tikzstyle{decision} = [ diamond, draw, fill=blue!10, text width=4.5em, text badly centered, node distance=2cm, inner sep-0pt]  
\tikzstyle{block} = [ rectangle, draw, fill=blue!10, text width=5em, text badly centered, rounded corners, minimum height=4em]  
\tikzstyle{line} = [ draw, -latex']  
%\tikzstyle{terminator} = [ draw, ellipse, fill=red!20, node distance=3cm, minimum height=2em]
\tikzstyle{terminator} = [rectangle, draw, fill=blue!10, text width=5em, text badly centered, rounded corners, minimum height=4em]  
\begin{center}
\begin{tikzpicture}[node distance=3cm, auto]  
  \node [terminator]  (start)  {Parse tokens};  
  \node [block, right of=start]  (number) {Number rules};  
  \node [block, right of=number] (disambigue) {Disambigue square brackets}; 
  \node [block, right of=disambigue] (return) {Return result}; 
 
  \path [line] (start)  -- (number);  
  \path [line] (number) -- (disambigue);  
  \path [line] (disambigue) -- (return); 
\end{tikzpicture}
\end{center}
\caption{Parsing procedure}
\label{fig:ConceptParserFlow}
\end{figure}


\subsection{Data structure and data types}\label{sec:ConceptParserDataStructure}
To build the representative data structure, data types that represent the data stored in the \ac{TPTP} syntax have to be defined.
The following section describes the data structure and data types that are used and created by the parser in the parsing process.

Atomary data types

\subsubsection{Terminal symbol}
The terminal symbol data type has one attribute, which is the name of the terminal symbol it represents.

-todo Production Property

\subsubsection{Nonterminal symbol}
Analogue to the terminal symbol data type, the nonterminal symbol also has its name as an attribute.

%Composite data types

\subsubsection{Rules}
%\subsubsection{Grammar Expression}
A rule consists of the nonterminal symbol name which is produced, a production list and a position.
The position denotes at which position in the \ac{TPTP} syntax the rule was listed.
This information is needed to maintain the original order of the rules when printing the reduced syntax.\\
For each rule type (see table \ref{tbl:ConceptTPTPProductionSymbols}) there is a data type.
This means that grammar, token, strict and macro rule data types are introduced.\\
Listing \ref{lst:ConceptParserGrammarExpression} contains an example of a line in a \ac{TPTP} syntax file that is represented by the grammar rule data type.
The nonterminal symbol name which is produced is <$tff \textunderscore formula$>.  The production list consists of two productions, as can be seen in the listing.
\begin{lstlisting}[basicstyle=\scriptsize	,caption= Example of a grammar expression,label= lst:ConceptParserGrammarExpression]
<tff_formula>          ::= <tff_logic_formula> | <tff_atom_typing
\end{lstlisting}
\subsubsection{Comment block}
A comment block is a list of consecutive comment lines.

\subsubsection{Production element}
A production element is either a terminal or nonterminal symbol. Additionally a production symbol has a production property.

\subsubsection{Production property}
The production property can take one of three values and denotes whether a production is optional, can be repeated any number of times or does not have any special property.
In the original \ac{TPTP} syntax file this was represented by square brackets or the repetition symbol.

\subsubsection{Production}
A production is one production alternative specified in any expression.
It consists of a list of production elements and has a production property. Productions can also be nested.
Therefore the list can also contain further productions

-show example

\subsubsection{Production property}
The production property represents three options.
A production and a production element is either allowed to be repeated multiple times, is optional or does not have any special production property.

\subsubsection{Productions list}
A productions list contains a list of productions where each production is one alternative in the description of an expression.

\subsubsection{XOR Productions list}
The XOR productions list represents multiple alternatives enclosed by parentheses. It contains a list of the alternate productions.
 
\subsubsection{Grammar list}
The grammar list is the top level data structure. It contains a list of all elements that were in the \ac{TPTP} syntax file.
This includes any type of rules (grammar, token, strict and macro) and comment blocks.

\subsection{Production rules}
When using the \ac{PLY} parser generator, production rules have to be defined.
The rules describe how the tokens are to be processed.

todo describe production rules

\subsection{Disambiguation of  square brackets}
As mentioned before, square brackets have different meanings depending on the rule type.
The idea to solve this problem is to treat all rules the same in the first processing step.
Square brackets would then be interpreted as denoting the optional production property.
This production property would then be selected for productions that are enclosed by square brackets for all types of rules.
In an additional processing step, after creating the grammar list each grammar and strict rule can be iterated, exchanging the production property optional by the square bracket terminal symbols.

todo vor- nachteile


The output of the parser is a list of the rules and the comments from the \ac{TPTP} syntax file.
\section{Graph generation}\label{sec:ConceptGraphGeneration}
The \ac{TPTP} syntax, extracted from the \ac{TPTP} syntax file, needs to be stored in a data structure that allows for modification and traversing.
The data structure that is used is a graph consisting of nodes. 
Every nonterminal symbol that is on the left side of a production in the \ac{TPTP} syntax is represented by a node and instantiates a defined class "Node" that has the following attributes:
\begin{itemize}
\item value: name of nonterminal symbol
\item productions list: productions list of nonterminal symbol (REFERENCE)
\item rule type:  rule type of nonterminal symbol
\item comment block: list of comments belonging to the nonterminal symbol
\item position: position of the production in the input file
\item children: list containing all children of a node
TODO A child is ..
\end{itemize} 

Starting with the start symbol, the graph is generated recursively. Iterating over each nonterminal symbol that is on the right side of a production rule, the corresponding node is identified. These nodes are then appended to the list of children of the nonterminal on the left side of the rule. The identified children may again have children. This process is repeated until a node has no children because there are only terminal symbols on the right side of the production rule.

Since it is possible for a nonterminal symbol to be on the right side as well as on the left side of the same production rule, a node can also be its own child. To avoid revisiting the same node infinitely, it is checked whether a node already has children so that it will not be visited again. This also improves the performance of the tool as a nonterminal symbol that has already been visited wont be visited again independent of circular dependencies.

Furthermore, the start symbol can have multiple rule types resulting in two start symbols. As it is not possible in the application to have two start symbols, a new start symbol is generated that implies the two original start symbols. 

The following example shows a production rule and the resulting list of children belonging to the node. Each production alternative(REFERENCE) has its own list of children. %This is relevant for checking whether a nonterminal is terminating.
\begin{verbatim}
Production rule: 
<disjunction>  ::=  <literal> | <disjunction><vline><literal> 
Output:
node.value: <disjunction>
node.ruleType: grammar
node.children: [[<literal>],[<disjunction>,<vline>,<literal>]]
\end{verbatim}

\section{Control file}\label{sec:ConceptControlFile}
A format for specifying the desired start symbol and blocked productions has to be developed.
Using a file-based configuration enables the user to store desired configurations and for example a manual selection in the graphical user interface is not necessary.
It also helps with using the command line interface, because there manual selection is not possible.
The file should be human-readable and -editable.\\
The format should be easy to parse and allow to specify all necessary information.
This includes the desired start symbol and all production rules that should be blocked.\\
The proposed way to describe this information is to:

\begin{itemize}%[noitemsep]
	\item define the desired start symbol in the first line.
	\item define blocked productions grouped by nonterminal symbol and production symbol separating each group by a new line.
	First defining the nonterminal symbol, then the production symbol and after that the index of the alternatives that should be blocked (indexing starts at zero). 
\end{itemize}

Identifying the production symbol is necessary because there may be a nonterminal symbol that has productions with more than one production symbol.\\
Listing \ref{lst:ConceptControlFile} contains a sample control file. In this file in the first line <$TPTP\_file$> is specified as start symbol.
The second line means, that the second grammar production alternative of the nonterminal symbol <$TPTP\_input$> should be disabled.
Analogue to that, the first, second, third and fifth grammar production alternative of the nonterminal symbol <$annotated\_formula$> are said to be disabled in line 3.

\begin{lstlisting}[caption= Example of a control file,label= lst:ConceptControlFile]
<TPTP_file>
<TPTP_input>,::=,1
<annotated_formula>,::=,0,1,2,5
\end{lstlisting}
This format is relatively easy to parse and also enables users to specify their desired start symbols and blocked productions without having to use the GUI.

pro: Specifying which production should be blocked, and not the ones should be kept, typically results in a significantly smaller file.
Storing the indexes of the productions that should be blocked offers that in case productions are renamed the control file would still be valid. On the other hand if productions are added or deleted from the original \ac{TPTP} syntax, the control file may have to be updated.

\section{Maintainig comments}\label{sec:ConceptMaintainingComments}
In the \ac{TPTP} syntax there are comments providing supplemental information about the language and its symbols and rules.
When generating a reduced grammar maintaining of comments is desired. This means that comments from the original language specification should be associated with the rule they belong to and if the rule is still present in the reduced grammar, also the comment should be.\\
Therefore a mechanism has to be designed for the association of comments to grammar rules.

Listing \ref{lst:ConceptComment_tptp} features an example of a comment in a \ac{TPTP} syntax file. This comment begins with a \textit{Top of Page} line which, in the HTML version of the \ac{TPTP} syntax, contains a hyperlink which leads to the beginning of the syntax file.
The next line contains a relevant comment.\\
\begin{lstlisting}[language=none, basicstyle=\scriptsize	,caption= Example of a comment in the \ac{TPTP} syntax,label= lst:ConceptComment_tptp]
%----Top of Page---------------------------------------------------------------
%----TFF formulae.
<tff_formula>          ::= <tff_logic_formula> | <tff_atom_typing> |
                           <tff_subtype> | <tfx_sequent>
\end{lstlisting}
todo check if listing is handled correctly

The heuristic matching comments to rules takes these \textit{Top of Page} lines into account.
When there is a \textit{Top of Page} line in between comment lines it generally also splits comments sematically. todo maybe proof
In listing \ref{lst:ConceptComment_tptp} can be seen that the comment in line 2 refers to the rule after.
Therefore it would be correct to associate the comment line after the \textit{Top of Page} line to the rule after.
Also, if there is one \textit{Top of Page} line in between multiple comment lines it is highly probable that the first part of the comment lines before the \textit{Top of Page} line refer to the rule before the comments and that the lines after the \textit{Top of Page} line refer to the rule after the comment lines.
This scenario can be seen in listing \ref{lst:Comment_split_example}. The \textit{Top of Page} line is in line 28 and the comment lines before refer to the rule before.
The comment line after refers to the rule after that line.
\begin{lstlisting}[language=none, basicstyle=\scriptsize	,caption=Example of comment lines split by a \textit{Top of Page} line in the \ac{TPTP} syntax,label= lst:Comment_split_example]
<formula_role>         :== axiom | hypothesis | definition | assumption |
                           lemma | theorem | corollary | conjecture |
                           negated_conjecture | plain | type |
                           fi_domain | fi_functors | fi_predicates | unknown
%----"axiom"s are accepted, without proof. There is no guarantee that the
%----axioms of a problem are consistent.
%----"hypothesis"s are assumed to be true for a particular problem, and are
%----used like "axiom"s.
%----"definition"s are intended to define symbols. They are either universally
%----quantified equations, or universally quantified equivalences with an
%----atomic lefthand side. They can be treated like "axiom"s.
%----"assumption"s can be used like axioms, but must be discharged before a
%----derivation is complete.
%----"lemma"s and "theorem"s have been proven from the "axiom"s. They can be
%----used like "axiom"s in problems, and a problem containing a non-redundant
%----"lemma" or theorem" is ill-formed. They can also appear in derivations.
%----"theorem"s are more important than "lemma"s from the user perspective.
%----"conjecture"s are to be proven from the "axiom"(-like) formulae. A problem
%----is solved only when all "conjecture"s are proven.
%----"negated_conjecture"s are formed from negation of a "conjecture" (usually
%----in a FOF to CNF conversion).
%----"plain"s have no specified user semantics.
%----"fi_domain", "fi_functors", and "fi_predicates" are used to record the
%----domain, interpretation of functors, and interpretation of predicates, for
%----a finite interpretation.
%----"type" defines the type globally for one symbol; treat as $true.
%----"unknown"s have unknown role, and this is an error situation.
%----Top of Page---------------------------------------------------------------
%----THF formulae.
<thf_formula>          ::= <thf_logic_formula> | <thf_atom_typing> |
                           <thf_subtype> | <thf_sequent>
\end{lstlisting}
heuristic:
comments near the rule they refer to
associate comment with r


The flow chart in figure \ref{fig:ConceptMaintainingComments} shows the process of matching comment blocks, that are consecutive comment lines (see section \ref{sec:ConceptParserDataStructure}),  to rules.
First, the comment block is split into multiple separate comment blocks by using \textit{Top of Page} lines as separators.
\begin{itemize}%[noitemsep]
	\item If this results in no comment blocks the comment block consisted only of one line which was a \textit{Top of Page} line.
	Then no comment block has to be associated to a rule because \textit{Top of Page} lines are not relevant.
	\item If this results in one comment block, that means that no \textit{Top of Page} line was present in the comment block and the comment block is associated with the rule after, if the comment block is not at the end of the file.
	If it is at the end of the file it is associated with the rule before. todo why
	\item If this results in two comment blocks, one \textit{Top of Page} line was present.
	Then the comment block before the \textit{Top of Page} line is associated with the rule before when possible.
	If this comment block is at the beginning of the file it is associated with the rule after.
	The comment block after the \textit{Top of Page} line is associated with the rule after.
	If it is at the end of the file it is associated with the rule before.
\end{itemize}
The case of three or more comment blocks after splitting the original comment block is not featured in the flow chart.
This case does not occur in the \ac{TPTP} syntax version 7.3.0.
Therefore it is not particularly relevant.
Since it might occur in a future version of the \ac{TPTP} syntax it is handled by merging all comment blocks starting from the second and then following the procedure of two comment blocks in the flow chart in figure \ref{fig:ConceptMaintainingComments}.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{images/Concept_maintaining_comments.pdf}
\caption{Maintaining comments flow chart}
\label{fig:ConceptMaintainingComments}
\end{figure}
todo describe split comment block by top of page

todo explain could use content of comment
\section{Generation of a sub-syntax}\label{sec:ConceptGenerateReducedGrammar}
This section covers the concept of how a sub-syntax can be computed from the original syntax.
The original syntax is represented by a grammar graph (see section \ref{sec:ConceptGraphGeneration}) and the information on what part of the grammar should be extracted is specified in the control file.
For that, three steps must be performed:

\begin{enumerate}%[noitemsep]
	\item The blocked productions specified in the control file must be disabled and therefore the corresponding transitions must be removed from the grammar graph.
	\item The remaining reachable part of the grammar must be computed.
	\item Starting from the still reachable part of the grammar, non terminating productions must be removed.
\end{enumerate}

bei tree building temporäres startsymbol nutzen (da mehrere Startsymbole möglich)
\subsection{Removing of blocked productions}

\subsection{Determination of the remaining reachable productions}  
dynamic programming
\subsection{Determination of the remaining terminating productions}

\section{GUI}\label{sec:ConceptGUI}
In this section ...
The graphical user interface should display the grammar similar to the original language grammar specification file.
It should also be possible to make selections in the GUI instead of having to use a control file. 
-show rules similar to file
Selection of a new start symbol and productions that should be possible in the GUI and also with the import of a control file.

-show extracted grammar
-export exported grammar
-include + toggle comments -> algorithm
-import/export control file
extra:
-web import
\subsection{Menu}\label{sec:ConceptGUIMenu}
\subsection{Rules display}\label{sec:ConceptRulesDisplay}


\section{Command-line interface}\label{sec:ConceptCommandLineInterface}
The goal of the command-line interface is to provide means for convenient automation of sub-syntax extraction. It should take a \ac{TPTP} syntax file and a control file as input and output the resulting sub-syntax. Also basic help information should be accessible over the command-line interface. 

-command-line interface allows for automation with scripts for repeated tasks
-basic functionality extract sub-syntax by providing base syntax file and control file, 
-output sub-syntax with input control file
-provide basic information with help menu
-more complex actions like control file generation can more comfortably be done by using gui